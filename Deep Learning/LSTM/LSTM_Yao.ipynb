{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dsets\n",
    "import torch.utils.data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7576])\n"
     ]
    }
   ],
   "source": [
    "# 设置一个随机数,使我们可以每次均获得相同的随机数\n",
    "torch.manual_seed(1)\n",
    "print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 28\n",
    "INPUT_SIZE = 28\n",
    "LR = 0.01\n",
    "DOWNLOAD_MNIST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the train set of the minist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dsets.MNIST(\n",
    "    root='./data/',\n",
    "    train = True,\n",
    "    transform = torchvision.transforms.ToTensor(),\n",
    "    download = DOWNLOAD_MNIST,\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch processing data of the train data\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the test set of the minist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define test set\n",
    "testset = tv.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False,\n",
    "    download=DOWNLOAD_MNIST ,\n",
    "    transform=torchvision.transforms.ToTensor())\n",
    "testset[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the batch processing dataset\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the images are:  torch.Size([64, 1, 28, 28])\n",
      "the labels are:  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# show the function of the dataloader, we can understand that  every batch return the inputs and the labels. \n",
    "# The size of inputs is [64, 1, 28, 28], that means we have 64 samples, every sample is a figure, and it is [1 , 28 , 28].\n",
    "# In the meanwhile, we get the labels that are a 1 demension tensor including 64 elements.\n",
    "for data in testloader:\n",
    "    images , labels = data\n",
    "    print(\"the images are: \" , images.size())\n",
    "    print(\"the labels are: \" , labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面cell中定义的这个LSTM的参数的一些解释：**\n",
    "- input_size为28，意思是输入的特征维数是28；\n",
    "- hidden_size为64，意思是输出的特征维数是64；\n",
    "- num_layers为1，意思是隐藏层只有1层；\n",
    "- batch_first为True,意思是输入输出的第0个维度是batch_size.\n",
    "- input是一个三维的tensor,各个维度的意思分别是：\n",
    "\n",
    "    输入数据格式：\n",
    "    \n",
    "    input(seq_len, batch, input_size)\n",
    "    \n",
    "    h0(num_layers * num_directions, batch, hidden_size)\n",
    "    \n",
    "    c0(num_layers * num_directions, batch, hidden_size)\n",
    "    \n",
    "- output是一个三维的tensor，各个维度的意思分别是：\n",
    "\n",
    "    输出数据格式：\n",
    "    \n",
    "    output(seq_len, batch, hidden_size * num_directions)\n",
    "    \n",
    "    hn(num_layers * num_directions, batch, hidden_size)\n",
    "    \n",
    "    cn(num_layers * num_directions, batch, hidden_size)\n",
    "    \n",
    "*在我们这个例子中，input's size is $64*28*28$，也就是说一个batch有64个samples，序列长度是28，当然序列长度其实是可变的，不一定是固定的，这也是为什么模型中没有给出seq_len这个参数的原因，因为它是可变的。最后输入的特征维数是28，也就是说把一张图片的每一行像素作为一个序列的一个单元。output's size is $64*28*64$,也就是说一个batch有64个samples，序列长度是28，输出特征维数28.*\n",
    "**之后在本模型中还加了一个仿射层。**仿射层前将每个output的size从$64*28*64$变成了$64*64$，然后经过放射变换后就变成了$64*10$,64就是一个batch有64个样本的意思，10代表的意思是0,1,2,3，···，9各个数字的概率，取结果为最大概率的那个数字就是最后的结果了。\n",
    "我觉得取序列的任何一个理论上都可以的，取最后一个得到的准确率为95%，取序列的第一个准确率为11%，所以事实上只能取序列的最后一个，取序列的前面是会出现问题的。这是因为该模型实际上需要输入的是一个序列数据，在此我们便是把一张图片看成是一个序列，具体来说是每一行作为一个时刻，每一个时刻就是一行的28个像素，所以输入特征的维数是28，序列长度也是28，当然这是可以改变的，序列长度不一定要定长。所以自然应当是取序列中的最后一个向量作为放射层的输入。然后用双向的LSTM进行了实验，准确率有略微的上升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RNN model \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):  # 定义一些需要的参数或者说是组件\n",
    "        super(RNN , self).__init__()\n",
    " \n",
    "        self.rnn = nn.LSTM(  # 使用nn.Module中自带的LSTM模型进行分析\n",
    "            input_size = 28,  # 输入特征的维度是28\n",
    "            hidden_size = 64,  # 输出特征的维度是64\n",
    "            num_layers = 1,  # 隐藏层层数是1\n",
    "            batch_first = True,  # 第一个参数是batch\n",
    "        ) # rnn的参数里没有指定seq_len,因为seq_len是可变的。\n",
    " \n",
    "        self.out = nn.Linear(64,10)\n",
    " \n",
    "    def forward(self,x):  # 将init中定义的组件组合起来形成LSTM，并进行前向传播计算，反向传播会自动进行的不需要定义这个function.\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)\n",
    "#         print(1 , r_out.size())\n",
    "#         print(2 , r_out[:,-1,:].size())\n",
    "        out = self.out(r_out[:,-1,:])  # 只取序列中的最后一个进行分析。 \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7629, 0.7097, 0.1461],\n",
      "         [0.6418, 0.7792, 0.0941],\n",
      "         [0.0923, 0.4499, 0.2842]],\n",
      "\n",
      "        [[0.0705, 0.5491, 0.9103],\n",
      "         [0.7107, 0.6511, 0.5376],\n",
      "         [0.2022, 0.0587, 0.8673]]])\n",
      "tensor([[0.0923, 0.4499, 0.2842],\n",
      "        [0.2022, 0.0587, 0.8673]])\n",
      "tensor([[0.0705, 0.5491, 0.9103],\n",
      "        [0.7107, 0.6511, 0.5376],\n",
      "        [0.2022, 0.0587, 0.8673]])\n"
     ]
    }
   ],
   "source": [
    "# help you understand the meaning of out = self.out(r_out[:,-1,:]) \n",
    "a = torch.rand((2 , 3 , 3))\n",
    "print(a)\n",
    "print(a[:,-1,:])\n",
    "print(a[-1,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**see the defined LSTM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): LSTM(28, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(),lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the LSTM model, and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurancy of 1th epoch is : 95.210%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    rnn.train()  #  set the net as the train mode, which means the batch layer's parameters are stable and drop out is unworking!  \n",
    "    for step,(x,y) in enumerate(trainloader):\n",
    "        b_x = Variable(x.view(-1,28,28))\n",
    "#         print(b_x.size())\n",
    "        b_y = Variable(y)\n",
    " \n",
    "        optimizer.zero_grad()  # Gradient clearing\n",
    "        output = rnn(b_x) # 前向传播\n",
    "        loss = loss_func(output,b_y)  # 计算loss function\n",
    "        \n",
    "        loss.backward() # backward propagation, prepare to update the parameters\n",
    "        optimizer.step()  # update the parameters of the model.\n",
    " \n",
    "    rnn.eval()  # set the model as the test mode.\n",
    "    # if one epoch has been over, then test the accuaccy, and print it.\n",
    "    with torch.no_grad(): # do not need calculate the gradient\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for data in testloader:\n",
    "            images , labels = data\n",
    "            images = Variable(images.view(-1,28,28))\n",
    "#             images , labels = images.to(device) , labels.to(device)\n",
    "            outputs = rnn(images)\n",
    "            # set the input as the label that get the highest score\n",
    "            _ , predicted = torch.max(outputs.data , 1)  # return the max value of one row and the index of the max value\n",
    "            total = total + labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print(\"The accurancy of {}th epoch is : {:.3f}%\".format(epoch + 1 , correct / total * 100))\n",
    "\n",
    "        '''\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(test_x.view(-1,28,28))\n",
    "            pred_y = torch.max(test_output,1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y)/float(test_y.size(0))\n",
    "            print('Epoch: ',epoch, '| train loss:%.4f' %loss.data[0],'| test accuracy:%.2f' %accuracy)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the LSTM more, know more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 10])\n",
      "tensor([[-3.3535e+00,  1.1583e+00, -6.6715e-01,  1.3278e+00, -6.2588e-01,\n",
      "         -1.9954e+00, -5.5185e+00,  8.7044e+00, -3.7987e+00,  2.1977e+00],\n",
      "        [-3.6073e+00,  8.6772e-01,  1.0681e+01, -1.1031e+00, -2.0871e+00,\n",
      "         -4.5243e-01,  5.9701e-02,  7.8678e-01,  2.5038e-01, -4.5078e+00],\n",
      "        [-3.7639e+00,  1.1737e+01, -3.7288e-01, -1.7628e+00,  7.2247e-03,\n",
      "         -3.6377e-01, -2.3686e+00, -1.0372e+00, -1.1450e+00, -2.9703e+00],\n",
      "        [ 5.6501e+00, -2.9123e+00, -7.8421e-01, -2.1026e+00, -1.7124e+00,\n",
      "         -2.7104e-02,  1.9857e+00, -3.8162e+00,  3.7686e-01,  4.9845e-01],\n",
      "        [-3.0889e+00, -2.2300e+00, -1.2888e+00, -4.0178e+00,  9.7358e+00,\n",
      "         -2.7764e+00, -9.8048e-01, -2.1520e-01, -1.7955e+00,  9.1425e-01],\n",
      "        [-3.5080e+00,  1.1271e+01, -1.2221e+00, -1.5767e+00,  8.5013e-02,\n",
      "         -5.6014e-01, -3.3541e+00, -1.1469e-01, -1.6149e+00, -2.1192e+00],\n",
      "        [-2.4550e+00, -1.9412e+00, -9.8498e-01, -2.9590e+00,  8.1103e+00,\n",
      "         -1.4231e+00, -1.6998e+00, -1.6469e-04, -1.3222e+00,  3.8615e-01],\n",
      "        [-3.5941e+00, -3.4304e+00, -8.2435e-01, -1.2909e+00,  2.5269e+00,\n",
      "         -5.0413e-01, -2.2627e+00, -2.5500e+00, -4.3325e-01,  5.7674e+00],\n",
      "        [-3.3101e+00, -1.1617e+00, -8.1378e-01, -1.2616e+00, -2.0525e-01,\n",
      "          4.4630e+00,  8.5316e-01,  9.2451e-01,  1.1068e+00, -2.2096e+00],\n",
      "        [-2.6750e+00, -5.8064e+00, -3.9857e+00,  6.0202e-01,  3.6365e-01,\n",
      "         -4.7044e-03, -4.4004e+00,  1.8839e+00, -2.5103e+00,  8.4722e+00],\n",
      "        [ 9.0700e+00, -1.7942e+00, -1.9241e+00, -1.7861e+00, -1.3833e+00,\n",
      "         -7.3518e-01,  9.2772e-01, -3.8029e+00,  1.2455e-01, -1.0615e+00],\n",
      "        [ 2.0122e+00, -8.2073e-02, -1.1668e+00, -3.4614e+00, -1.6687e+00,\n",
      "          9.1958e-01,  9.2649e+00, -4.9351e+00,  3.3920e+00, -5.0187e+00],\n",
      "        [-2.2914e+00, -5.8644e+00, -3.5154e+00,  8.8885e-01, -1.1545e-01,\n",
      "         -4.9330e-01, -3.2716e+00, -4.1537e-01, -2.5300e+00,  8.9999e+00],\n",
      "        [ 8.0871e+00, -3.7368e+00, -2.9167e+00, -1.6242e+00, -2.1514e+00,\n",
      "          5.4306e-01,  1.7602e+00, -3.1020e+00,  3.0855e-01, -4.7190e-01],\n",
      "        [-3.1725e+00,  6.7316e+00, -7.8419e-01,  1.3108e-01,  1.2278e+00,\n",
      "         -8.0129e-01, -3.2774e+00,  2.5839e-01, -1.3826e+00, -2.6745e-01],\n",
      "        [-2.0886e+00, -4.7992e+00, -2.5166e+00,  2.6329e+00, -2.9551e+00,\n",
      "          7.8077e+00, -7.4010e-01, -3.7095e+00, -2.1558e+00,  2.4336e+00],\n",
      "        [-1.6094e+00, -5.2500e+00, -2.6530e+00,  4.9361e-01, -3.8170e-01,\n",
      "         -6.7438e-01, -3.5223e+00, -1.0095e+00, -1.8359e+00,  7.6457e+00],\n",
      "        [-3.5501e+00,  1.4806e-01, -1.0575e+00,  1.3295e+00, -8.8885e-02,\n",
      "         -2.0578e+00, -5.4055e+00,  8.2023e+00, -3.3022e+00,  2.4990e+00],\n",
      "        [-2.5975e+00, -3.3736e+00, -2.9040e+00,  5.3538e+00, -3.1498e+00,\n",
      "          3.1078e+00, -3.5534e+00, -3.4060e+00,  1.6686e+00,  2.4455e+00],\n",
      "        [-3.3559e+00, -2.7764e+00, -5.6110e-01, -3.5187e+00,  9.3757e+00,\n",
      "         -1.8588e+00, -1.6539e+00, -2.7395e-01, -1.2437e+00,  6.3486e-01],\n",
      "        [-2.2489e+00, -4.7042e+00, -4.0769e+00,  2.0980e+00,  3.0229e-01,\n",
      "          1.2060e+00, -3.9948e+00,  3.2649e+00, -4.2489e+00,  6.2598e+00],\n",
      "        [-3.3821e-01,  1.1905e+00,  4.0913e-01, -3.9589e+00, -1.2230e+00,\n",
      "          1.4316e+00,  9.5057e+00, -4.6056e+00,  1.6197e+00, -3.7818e+00],\n",
      "        [ 9.9047e-01,  3.7559e-01, -1.2287e+00, -4.1795e+00,  2.4770e+00,\n",
      "         -5.7618e-02,  9.3703e+00, -5.3546e+00, -1.1836e+00, -2.7457e+00],\n",
      "        [-2.4742e+00, -5.5026e+00, -3.6156e+00,  1.5318e+00, -3.0055e+00,\n",
      "          9.5120e+00, -1.1370e+00, -2.8404e+00, -1.5707e+00,  1.3013e+00],\n",
      "        [-3.7027e+00, -2.4605e+00, -4.6947e-01, -4.0420e+00,  9.8569e+00,\n",
      "         -2.4792e+00, -1.5794e+00, -5.2753e-01, -1.3987e+00,  1.1065e+00],\n",
      "        [ 8.4564e+00,  1.6149e-03, -5.9436e-01, -2.5058e+00, -4.2586e-01,\n",
      "         -3.6614e+00,  3.1508e+00, -3.3544e+00,  5.6428e-01, -2.1210e+00],\n",
      "        [-8.3296e-01, -7.6915e-01, -1.4643e+00,  1.1315e+00, -8.6924e-01,\n",
      "         -5.1553e-02, -2.3381e+00,  4.4972e+00, -2.2939e+00,  2.3062e+00],\n",
      "        [-3.4734e+00, -2.6280e+00, -5.2148e-01, -4.1569e+00,  1.0171e+01,\n",
      "         -2.3681e+00, -1.3439e+00,  5.3716e-02, -1.6032e+00,  4.0177e-01],\n",
      "        [ 9.6452e+00, -3.2728e+00, -1.6855e+00, -1.0622e+00, -2.6646e+00,\n",
      "         -6.1735e-01,  1.7421e-01, -2.5017e+00,  9.4258e-01, -1.1678e+00],\n",
      "        [-3.5286e+00,  9.5426e+00, -7.5286e-03, -1.0491e+00,  1.6057e-01,\n",
      "         -1.4940e+00, -2.2920e+00, -2.2363e-01, -1.0365e+00, -1.6135e+00],\n",
      "        [-2.3595e+00, -1.2789e+00,  8.2843e-01,  9.0778e+00, -4.6781e+00,\n",
      "          8.4408e-01, -4.8551e+00,  7.4480e-01, -8.2019e-01, -6.4943e-01],\n",
      "        [-3.6131e+00,  6.7212e+00, -3.7099e-01, -8.9665e-01,  1.3406e+00,\n",
      "         -1.7518e+00, -2.3462e+00, -1.2963e-01, -1.4539e+00,  1.0229e-02],\n",
      "        [-1.7464e+00, -2.2556e+00,  8.1276e-01,  9.3479e+00, -5.1687e+00,\n",
      "          1.4055e+00, -4.2549e+00, -5.7210e-01,  2.0620e-01, -6.3831e-01],\n",
      "        [-8.4253e-01, -1.5851e+00, -9.1988e-01, -5.0030e+00,  8.8220e+00,\n",
      "         -2.9352e+00, -5.6178e-01,  5.5585e-02, -1.6248e+00, -2.4395e-01],\n",
      "        [-4.3332e+00,  1.0094e+00,  7.6780e-01,  1.3221e+00, -4.7636e-01,\n",
      "         -2.8288e+00, -5.9517e+00,  8.2899e+00, -3.0570e+00,  1.9628e+00],\n",
      "        [-2.1583e+00,  4.3622e-01,  1.0123e+01, -6.6263e-01, -1.9091e+00,\n",
      "         -1.8036e+00, -1.4237e+00,  2.0095e+00,  3.1666e-01, -3.7605e+00],\n",
      "        [-3.2537e+00, -7.2813e-01,  1.2400e+00,  7.9793e-01, -2.3012e-01,\n",
      "         -2.2158e+00, -4.1986e+00,  7.8804e+00, -1.6775e+00,  1.1244e+00],\n",
      "        [-3.2112e+00,  8.5131e+00, -4.2081e-01, -8.8585e-01,  1.1904e-01,\n",
      "         -1.8746e+00, -2.6994e+00,  4.0357e-01, -1.1373e+00, -6.0873e-01],\n",
      "        [-2.2279e+00,  5.3989e-01,  8.9946e+00, -8.5408e-01, -2.2562e+00,\n",
      "         -5.9048e-01,  4.2233e-01,  4.9893e-01,  9.2249e-01, -4.8136e+00],\n",
      "        [-3.1611e+00,  1.0053e+01,  1.4093e-01, -1.4872e+00, -1.4192e-01,\n",
      "         -9.6275e-01, -2.6295e+00,  8.6585e-01, -5.5795e-01, -2.3291e+00],\n",
      "        [-4.5641e+00,  9.8197e+00, -1.6802e+00, -1.1210e+00,  1.9654e+00,\n",
      "          3.1698e-01, -3.5446e+00, -5.7192e-01, -2.4430e+00, -1.1374e+00],\n",
      "        [-4.2452e+00,  3.7789e-01,  5.8554e-01,  1.2464e+00, -4.7640e-01,\n",
      "         -1.7696e+00, -5.1289e+00,  8.5790e+00, -2.8609e+00,  1.7163e+00],\n",
      "        [-2.7253e+00, -2.5383e+00, -1.3294e+00, -3.5733e+00,  9.1065e+00,\n",
      "         -1.7397e+00, -1.8497e+00, -2.2641e-01, -1.2793e+00,  5.8342e-01],\n",
      "        [-2.4911e+00,  7.8107e-01,  1.0109e+01, -1.7730e+00, -1.1383e+00,\n",
      "         -1.8953e+00,  9.7928e-01, -2.7031e-01,  2.7176e-01, -3.4522e+00],\n",
      "        [-2.0016e+00, -6.8991e-01,  7.0492e-01,  8.0123e+00, -4.0460e+00,\n",
      "          1.2158e+00, -4.1885e+00, -5.6117e-01, -2.7936e-01, -1.3383e+00],\n",
      "        [-2.5117e+00, -4.4300e+00, -3.0206e+00,  2.3205e+00, -3.5044e+00,\n",
      "          9.4805e+00, -7.3868e-01, -3.2366e+00, -1.9955e+00,  9.6216e-01],\n",
      "        [-3.6858e+00,  5.4057e+00, -6.5265e-01, -4.7247e-01,  1.0538e+00,\n",
      "         -6.8737e-01, -1.3914e+00, -7.0027e-01,  3.0000e-02, -4.8321e-01],\n",
      "        [-2.4686e+00,  3.7282e-02,  1.0304e+01, -1.3441e+00, -1.2771e+00,\n",
      "         -1.7273e+00,  3.6894e-02, -2.9244e-02,  1.0691e+00, -3.6632e+00],\n",
      "        [-2.3122e+00, -2.6205e+00, -2.3495e+00, -1.8688e+00,  7.0356e+00,\n",
      "         -1.5753e+00, -1.2775e+00,  3.9535e-01, -2.5639e+00,  1.0769e+00],\n",
      "        [-3.3686e+00, -2.3276e+00, -1.1078e+00, -4.1039e+00,  9.7136e+00,\n",
      "         -2.9331e+00, -1.0442e+00, -2.3731e-01, -2.0483e+00,  1.3319e+00],\n",
      "        [ 1.0800e+00,  2.2627e+00, -1.4541e+00, -4.4309e+00,  1.5660e-01,\n",
      "          1.7982e+00,  1.1091e+01, -5.8135e+00, -1.2950e-01, -4.5867e+00],\n",
      "        [-2.1317e+00, -1.1665e+00,  1.7256e+00,  9.4799e+00, -5.4618e+00,\n",
      "          1.0806e+00, -4.3287e+00, -9.0510e-02,  6.0569e-01, -2.1144e+00],\n",
      "        [-2.3374e+00, -5.3355e+00, -3.4452e+00,  9.5780e-01, -2.2843e+00,\n",
      "          9.5311e+00,  3.5886e-02, -3.2332e+00, -2.5013e+00,  1.8522e+00],\n",
      "        [-1.1563e+00, -5.5261e+00, -2.2863e+00,  2.4726e+00, -2.8386e+00,\n",
      "          6.4533e+00, -6.8765e-01, -3.9723e+00, -1.7911e+00,  2.8616e+00],\n",
      "        [ 1.8334e+00,  1.8450e+00, -1.2361e+00, -4.2017e+00,  2.3248e-01,\n",
      "          1.3820e+00,  1.0334e+01, -5.1969e+00,  9.8865e-01, -5.3470e+00],\n",
      "        [ 8.9521e+00, -2.0503e+00, -2.7940e+00, -1.7771e+00, -1.7422e+00,\n",
      "         -1.0565e+00,  1.6442e+00, -3.1586e+00,  1.6320e+00, -1.6688e+00],\n",
      "        [-3.1024e+00, -2.7671e+00, -1.2381e+00, -3.4046e+00,  9.1454e+00,\n",
      "         -2.5977e+00, -8.9836e-01,  6.5844e-02, -1.9151e+00,  6.6536e-01],\n",
      "        [-4.0057e+00,  1.0842e+01, -1.4044e+00, -1.6609e+00,  9.4263e-01,\n",
      "         -3.2565e-01, -3.7118e+00,  4.0728e-01, -1.7789e+00, -1.9438e+00],\n",
      "        [-1.6576e+00, -5.4953e+00, -2.6583e+00,  1.2227e+00, -5.4136e-01,\n",
      "          6.9346e-01, -3.3601e+00, -2.4331e+00, -1.7647e+00,  7.5259e+00],\n",
      "        [-2.9916e+00, -4.2998e+00, -4.1158e+00, -8.4640e-01, -2.9335e-01,\n",
      "          6.0777e+00, -1.7704e+00, -1.0133e+00,  1.0774e+00,  1.5903e+00],\n",
      "        [-1.3417e+00, -1.2845e+00, -1.9402e+00,  1.8710e+00, -9.2971e-01,\n",
      "          6.3164e-02, -3.6983e+00,  5.7055e+00, -2.7383e+00,  2.8176e+00],\n",
      "        [ 4.2640e-01, -2.2235e+00, -7.4193e-02, -4.6201e-01, -2.4122e+00,\n",
      "         -9.7877e-01, -7.9083e-01, -1.8516e+00,  7.5398e+00, -2.2139e+00],\n",
      "        [-3.4530e+00, -2.4201e+00, -2.5721e+00, -1.4994e+00,  5.3067e-01,\n",
      "          3.0178e+00, -3.5943e-01, -1.2879e+00, -1.4886e+00,  3.8528e+00],\n",
      "        [-1.9252e+00, -1.1359e+00,  1.4303e+00,  7.4115e+00, -4.4240e+00,\n",
      "         -4.1968e-02, -5.1764e+00,  4.6805e-01,  6.0898e-01, -1.5700e+00]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in testloader:\n",
    "    images , labels = data\n",
    "    images = Variable(images.view(-1,28,28))\n",
    "    break\n",
    "print(images.size())\n",
    "print(labels.size())\n",
    "output = rnn(images)\n",
    "print(output.size())\n",
    "print(output)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the Bidirectional RNN\n",
    "\n",
    "试一试双向RNN做手写数字识别的效果，看会不会准确率进一步上升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RNN model \n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self):  # 定义一些需要的参数或者说是组件\n",
    "        super(BiRNN , self).__init__()\n",
    " \n",
    "        self.birnn = nn.LSTM(  # 使用nn.Module中自带的LSTM模型进行分析\n",
    "            input_size = 28,  # 输入特征的维度是28\n",
    "            hidden_size = 64,  # 输出特征的维度是64\n",
    "            num_layers = 1,  # 隐藏层层数是1\n",
    "            batch_first = True,  # 第一个参数是batch\n",
    "            bidirectional=True  # 双向RNN\n",
    "        ) # rnn的参数里没有指定seq_len,因为seq_len是可变的。\n",
    " \n",
    "        self.out = nn.Linear(128,10)\n",
    " \n",
    "    def forward(self,x):  # 将init中定义的组件组合起来形成LSTM，并进行前向传播计算，反向传播会自动进行的不需要定义这个function.\n",
    "        r_out, (h_n, h_c) = self.birnn(x, None)\n",
    "#         print(1 , r_out.size())\n",
    "#         print(2 , r_out[:,-1,:].size())\n",
    "        out = self.out(r_out[:,-1,:])  # 只取序列中的最后一个进行分析。 \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (birnn): LSTM(28, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "birnn = BiRNN()\n",
    "print(birnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(birnn.parameters(),lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accurancy of 1th epoch is : 97.060%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    birnn.train()  #  set the net as the train mode, which means the batch layer's parameters are stable and drop out is unworking!  \n",
    "    for step,(x,y) in enumerate(trainloader):\n",
    "        b_x = Variable(x.view(-1,28,28))\n",
    "#         print(b_x.size())\n",
    "        b_y = Variable(y)\n",
    " \n",
    "        optimizer.zero_grad()  # Gradient clearing\n",
    "        output = birnn(b_x) # 前向传播\n",
    "        loss = loss_func(output,b_y)  # 计算loss function\n",
    "        \n",
    "        loss.backward() # backward propagation, prepare to update the parameters\n",
    "        optimizer.step()  # update the parameters of the model.\n",
    " \n",
    "    birnn.eval()  # set the model as the test mode.\n",
    "    # if one epoch has been over, then test the accuaccy, and print it.\n",
    "    with torch.no_grad(): # do not need calculate the gradient\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        for data in testloader:\n",
    "            images , labels = data\n",
    "            images = Variable(images.view(-1,28,28))\n",
    "#             images , labels = images.to(device) , labels.to(device)\n",
    "            outputs = birnn(images)\n",
    "            # set the input as the label that get the highest score\n",
    "            _ , predicted = torch.max(outputs.data , 1)  # return the max value of one row and the index of the max value\n",
    "            total = total + labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "        print(\"The accurancy of {}th epoch is : {:.3f}%\".format(epoch + 1 , correct / total * 100))\n",
    "\n",
    "        '''\n",
    "        if step % 50 == 0:\n",
    "            test_output = rnn(test_x.view(-1,28,28))\n",
    "            pred_y = torch.max(test_output,1)[1].data.numpy().squeeze()\n",
    "            accuracy = sum(pred_y == test_y)/float(test_y.size(0))\n",
    "            print('Epoch: ',epoch, '| train loss:%.4f' %loss.data[0],'| test accuracy:%.2f' %accuracy)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用双向LSTM确实使得准确率有进一步的提升，基本上是从95%提升到了96%，这是因为双向的RNN不仅可以考虑到过去时刻的信息，还可以考虑到之后时刻的信息。不过在图像识别中这种提升不明显。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
